# ğŸ¦ Neo-Bank Credit Scoring Model
# CrÃ©ation d'un modÃ¨le 
# OBJECTIF: ModÃ¨le qui fonctionne sur Streamlit Cloud 

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
import pickle
import warnings
import os
warnings.filterwarnings('ignore')

print("ğŸš€ CRÃ‰ATION MODÃˆLE NEO-BANK ")
print("="*50)

# VÃ©rification de la version
import sklearn
print(f"ğŸ“Œ Version scikit-learn: {sklearn.__version__}")

# 1. CHARGEMENT DES VRAIES DONNÃ‰ES
print("\nğŸ“Š CHARGEMENT DES VRAIES DONNÃ‰ES")
print("="*40)

# Chargement du dataset d'entraÃ®nement
print("ï¿½ Chargement de application_train.csv...")
try:
    df_train = pd.read_csv('data/application_train.csv')
    print(f"âœ… Dataset chargÃ©: {df_train.shape}")
    print(f"ğŸ“Š Taux de dÃ©faut rÃ©el: {df_train['TARGET'].mean():.1%}")
except Exception as e:
    print(f"âŒ Erreur de chargement: {e}")
    exit(1)

# Exploration rapide des colonnes
print(f"\nğŸ“‹ Colonnes disponibles: {len(df_train.columns)}")
print("ğŸ” PremiÃ¨res colonnes:", list(df_train.columns[:10]))

# SÃ©lection des features importantes pour le modÃ¨le
# On prend les colonnes principales qui existent probablement
feature_candidates = [
    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',
    'DAYS_BIRTH', 'DAYS_EMPLOYED',
    'CNT_FAM_MEMBERS', 'CNT_CHILDREN',
    'CODE_GENDER', 'NAME_EDUCATION_TYPE', 'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS'
]

# VÃ©rifier quelles colonnes existent vraiment
existing_features = []
for col in feature_candidates:
    if col in df_train.columns:
        existing_features.append(col)
        print(f"âœ… {col}")
    else:
        print(f"âŒ {col} - non trouvÃ©e")

print(f"\nğŸ“Š Features retenues: {len(existing_features)}")

# PrÃ©paration des donnÃ©es
df = df_train[existing_features + ['TARGET']].copy()

# Conversion des jours en annÃ©es pour DAYS_BIRTH et DAYS_EMPLOYED
if 'DAYS_BIRTH' in df.columns:
    df['AGE_YEARS'] = (-df['DAYS_BIRTH'] / 365.25).astype(int)
    df = df.drop('DAYS_BIRTH', axis=1)

if 'DAYS_EMPLOYED' in df.columns:
    # Gestion des valeurs aberrantes (365243 = pas d'emploi)
    df['EMPLOYMENT_YEARS'] = -df['DAYS_EMPLOYED'] / 365.25
    df.loc[df['EMPLOYMENT_YEARS'] > 50, 'EMPLOYMENT_YEARS'] = 0  # Pas d'emploi
    df['EMPLOYMENT_YEARS'] = df['EMPLOYMENT_YEARS'].clip(0, 40)
    df = df.drop('DAYS_EMPLOYED', axis=1)

# Encodage des variables catÃ©gorielles
categorical_cols = ['CODE_GENDER', 'NAME_EDUCATION_TYPE', 'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS']
for col in categorical_cols:
    if col in df.columns:
        df[f'{col}_ENCODED'] = pd.Categorical(df[col]).codes
        df = df.drop(col, axis=1)

# Calcul de ratios financiers
if 'AMT_INCOME_TOTAL' in df.columns and 'AMT_CREDIT' in df.columns:
    df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL'].replace(0, 1)

if 'AMT_INCOME_TOTAL' in df.columns and 'AMT_ANNUITY' in df.columns:
    df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL'].replace(0, 1)

# Nettoyage des donnÃ©es
print("\nğŸ§¹ NETTOYAGE DES DONNÃ‰ES")
print("="*25)

# Suppression des valeurs manquantes
print(f"ğŸ“Š Valeurs manquantes avant: {df.isnull().sum().sum()}")
df = df.dropna()
print(f"ğŸ“Š Valeurs manquantes aprÃ¨s: {df.isnull().sum().sum()}")
print(f"ğŸ“Š Ã‰chantillons restants: {len(df)}")

# Limitation du dataset si trop grand (pour compatibilitÃ©)
if len(df) > 10000:
    df = df.sample(n=10000, random_state=42)
    print(f"ğŸ“Š Dataset rÃ©duit Ã : {len(df)} Ã©chantillons")

# DÃ©finition des features finales
feature_names = [col for col in df.columns if col != 'TARGET']
print(f"\nğŸ“‹ Features finales: {len(feature_names)}")
for i, feature in enumerate(feature_names, 1):
    print(f"  {i:2d}. {feature}")

print(f"âœ… Dataset crÃ©Ã©: {df.shape}")
print(f"ğŸ“Š Taux de dÃ©faut: {df['TARGET'].mean():.1%}")
print(f"ğŸ“‹ Variables: {len(feature_names)} features")

# VÃ©rification des donnÃ©es
print("\nğŸ“ˆ STATISTIQUES DU DATASET")
print("="*30)
print("Revenus moyens:", f"{df['AMT_INCOME_TOTAL'].mean():,.0f}â‚¬")
print("CrÃ©dit moyen:", f"{df['AMT_CREDIT'].mean():,.0f}â‚¬")
print("Ratio crÃ©dit/revenus moyen:", f"{df['CREDIT_INCOME_RATIO'].mean():.1f}x")
print("Ã‚ge moyen:", f"{df['AGE_YEARS'].mean():.0f} ans")
print("AnciennetÃ© moyenne:", f"{df['EMPLOYMENT_YEARS'].mean():.1f} ans")

# 2. PRÃ‰PARATION DES DONNÃ‰ES POUR L'ENTRAÃNEMENT
print("\nğŸ¯ PRÃ‰PARATION POUR L'ENTRAÃNEMENT")
print("="*40)

# SÃ©paration des features et de la cible
X = df[feature_names].copy()
y = df['TARGET'].copy()

print(f"ğŸ“Š Features shape: {X.shape}")
print(f"ğŸ¯ Target shape: {y.shape}")

# VÃ©rification des valeurs manquantes
print(f"ğŸ” Valeurs manquantes: {X.isnull().sum().sum()}")

# Division train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"âœ… Train set: {X_train.shape}")
print(f"âœ… Test set: {X_test.shape}")

# 3. STANDARDISATION DES DONNÃ‰ES
print("\nâš–ï¸ STANDARDISATION")
print("="*20)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… Standardisation appliquÃ©e")

# 4. ENTRAÃNEMENT DU MODÃˆLE
print("\nğŸ¤– ENTRAÃNEMENT DU MODÃˆLE")
print("="*30)

# ModÃ¨le Random Forest compatible scikit-learn 1.3.2
model = RandomForestClassifier(
    n_estimators=100,       # Nombre d'arbres
    max_depth=15,           # Profondeur max
    min_samples_split=10,   # Min Ã©chantillons pour split
    min_samples_leaf=5,     # Min Ã©chantillons par feuille
    random_state=42,        # ReproductibilitÃ©
    n_jobs=-1              # Utiliser tous les CPU
)

print("ğŸ”„ EntraÃ®nement en cours...")
model.fit(X_train_scaled, y_train)
print("âœ… ModÃ¨le entraÃ®nÃ©!")

# 5. Ã‰VALUATION DU MODÃˆLE
print("\nğŸ“Š Ã‰VALUATION")
print("="*15)

# PrÃ©dictions
train_score = model.score(X_train_scaled, y_train)
test_score = model.score(X_test_scaled, y_test)

print(f"ğŸ¯ Score Train: {train_score:.3f}")
print(f"ğŸ¯ Score Test: {test_score:.3f}")

# PrÃ©dictions pour AUC
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

print(f"ğŸ“ˆ AUC Score: {auc_score:.3f}")

# Importance des features
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nğŸ” TOP 5 FEATURES IMPORTANTES:")
for i, row in feature_importance.head(5).iterrows():
    print(f"  {row['feature']}: {row['importance']:.3f}")

# 6. SAUVEGARDE DU MODÃˆLE
print("\nğŸ’¾ SAUVEGARDE")
print("="*15)

# CrÃ©ation du dossier model
os.makedirs('model', exist_ok=True)

# Sauvegarde du modÃ¨le
with open('model/credit_model_v2.pkl', 'wb') as f:
    pickle.dump(model, f)
print("âœ… ModÃ¨le sauvegardÃ©: model/credit_model_v2.pkl")

# Sauvegarde du scaler
with open('model/scaler_v2.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("âœ… Scaler sauvegardÃ©: model/scaler_v2.pkl")

# Sauvegarde des noms de features
with open('model/feature_names_v2.pkl', 'wb') as f:
    pickle.dump(feature_names, f)
print("âœ… Features sauvegardÃ©es: model/feature_names_v2.pkl")

# Sauvegarde des encoders (vides pour ce modÃ¨le simple)
encoders = {
    'gender': None,
    'education': None,
    'income_type': None,
    'family_status': None
}

with open('model/encoders_v2.pkl', 'wb') as f:
    pickle.dump(encoders, f)
print("âœ… Encoders sauvegardÃ©s: model/encoders_v2.pkl")

# 7. TEST DU MODÃˆLE
print("\nğŸ§ª TEST DU MODÃˆLE")
print("="*20)

# Fonction de test adaptÃ©e aux vraies features
def test_prediction(income, credit_amount, annuity, age, employment_years, 
                   family_members=2, children=0, gender='M', education='Higher education', 
                   income_type='Working', family_status='Married'):
    """Test du modÃ¨le avec les vraies features"""
    
    # Calcul des ratios
    credit_income_ratio = credit_amount / income
    annuity_income_ratio = annuity / income
    
    # Encodage simple des variables catÃ©gorielles (basÃ© sur les valeurs typiques)
    gender_encoded = 1 if gender == 'M' else 0
    
    # Encodages simplifiÃ©s (vous pouvez ajuster selon vos donnÃ©es)
    education_mapping = {
        'Secondary / secondary special': 0,
        'Higher education': 1,
        'Incomplete higher': 2,
        'Lower secondary': 3,
        'Academic degree': 4
    }
    education_encoded = education_mapping.get(education, 1)
    
    income_type_mapping = {
        'Working': 0,
        'Commercial associate': 1,
        'Pensioner': 2,
        'State servant': 3
    }
    income_type_encoded = income_type_mapping.get(income_type, 0)
    
    family_status_mapping = {
        'Married': 0,
        'Single / not married': 1,
        'Civil marriage': 2,
        'Separated': 3,
        'Widow': 4
    }
    family_status_encoded = family_status_mapping.get(family_status, 0)
    
    # CrÃ©ation du vecteur avec exactement les 13 features du modÃ¨le
    # Dans l'ordre exact: AMT_INCOME_TOTAL, AMT_CREDIT, AMT_ANNUITY, CNT_FAM_MEMBERS, CNT_CHILDREN,
    # AGE_YEARS, EMPLOYMENT_YEARS, CODE_GENDER_ENCODED, NAME_EDUCATION_TYPE_ENCODED,
    # NAME_INCOME_TYPE_ENCODED, NAME_FAMILY_STATUS_ENCODED, CREDIT_INCOME_RATIO, ANNUITY_INCOME_RATIO
    
    features_test = np.array([[
        income,                    # AMT_INCOME_TOTAL
        credit_amount,             # AMT_CREDIT
        annuity,                   # AMT_ANNUITY
        family_members,            # CNT_FAM_MEMBERS
        children,                  # CNT_CHILDREN
        age,                       # AGE_YEARS
        employment_years,          # EMPLOYMENT_YEARS
        gender_encoded,            # CODE_GENDER_ENCODED
        education_encoded,         # NAME_EDUCATION_TYPE_ENCODED
        income_type_encoded,       # NAME_INCOME_TYPE_ENCODED
        family_status_encoded,     # NAME_FAMILY_STATUS_ENCODED
        credit_income_ratio,       # CREDIT_INCOME_RATIO
        annuity_income_ratio       # ANNUITY_INCOME_RATIO
    ]])
    
    print(f"ğŸ” Test avec {features_test.shape[1]} features (attendu: 13)")
    
    # Standardisation
    features_scaled = scaler.transform(features_test)
    
    # PrÃ©diction
    risk_proba = model.predict_proba(features_scaled)[0][1]
    risk_score = int(risk_proba * 100)
    
    return risk_score, risk_proba

# Tests avec diffÃ©rents profils
print("Profil 1 - Bon client:")
score1, proba1 = test_prediction(50000, 150000, 1200, 35, 5)
print(f"  Score: {score1}%, ProbabilitÃ©: {proba1:.3f}")

print("Profil 2 - Client risquÃ©:")
score2, proba2 = test_prediction(25000, 200000, 1800, 22, 0)
print(f"  Score: {score2}%, ProbabilitÃ©: {proba2:.3f}")

print("Profil 3 - Client excellent:")
score3, proba3 = test_prediction(80000, 200000, 1500, 40, 10)
print(f"  Score: {score3}%, ProbabilitÃ©: {proba3:.3f}")

print("\nğŸ‰ MODÃˆLE CRÃ‰Ã‰ AVEC SUCCÃˆS!")
print("="*30)
print("âœ… ModÃ¨le compatible scikit-learn 1.3.2")
print("âœ… Tous les fichiers sauvegardÃ©s dans model/")
print("âœ… PrÃªt pour Streamlit Cloud")
print(f"âœ… Version sklearn utilisÃ©e: {sklearn.__version__}")

print("\nğŸ“ Fichiers crÃ©Ã©s:")
print("  - model/credit_model_v2.pkl")
print("  - model/scaler_v2.pkl")
print("  - model/feature_names_v2.pkl")
print("  - model/encoders_v2.pkl")
