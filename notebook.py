# üè¶ Neo-Bank Credit Scoring Model
# Cr√©ation d'un mod√®le 
# OBJECTIF: Mod√®le qui fonctionne sur Streamlit Cloud 

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
import pickle
import warnings
import os
warnings.filterwarnings('ignore')

print("üöÄ CR√âATION MOD√àLE NEO-BANK ")
print("="*50)

# V√©rification de la version
import sklearn
print(f"üìå Version scikit-learn: {sklearn.__version__}")

# 1. CHARGEMENT DES VRAIES DONN√âES
print("\nüìä CHARGEMENT DES VRAIES DONN√âES")
print("="*40)

# Chargement du dataset d'entra√Ænement
print("ÔøΩ Chargement de application_train.csv...")
try:
    df_train = pd.read_csv('data/application_train.csv')
    print(f"‚úÖ Dataset charg√©: {df_train.shape}")
    print(f"üìä Taux de d√©faut r√©el: {df_train['TARGET'].mean():.1%}")
except Exception as e:
    print(f"‚ùå Erreur de chargement: {e}")
    exit(1)

# Exploration rapide des colonnes
print(f"\nüìã Colonnes disponibles: {len(df_train.columns)}")
print("üîù Premi√®res colonnes:", list(df_train.columns[:10]))

# S√©lection des features importantes pour le mod√®le
# On prend les colonnes principales qui existent probablement
feature_candidates = [
    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',
    'DAYS_BIRTH', 'DAYS_EMPLOYED',
    'CNT_FAM_MEMBERS', 'CNT_CHILDREN',
    'CODE_GENDER', 'NAME_EDUCATION_TYPE', 'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS'
]

# V√©rifier quelles colonnes existent vraiment
existing_features = []
for col in feature_candidates:
    if col in df_train.columns:
        existing_features.append(col)
        print(f"‚úÖ {col}")
    else:
        print(f"‚ùå {col} - non trouv√©e")

print(f"\nüìä Features retenues: {len(existing_features)}")

# Pr√©paration des donn√©es
df = df_train[existing_features + ['TARGET']].copy()

# Conversion des jours en ann√©es pour DAYS_BIRTH et DAYS_EMPLOYED
if 'DAYS_BIRTH' in df.columns:
    df['AGE_YEARS'] = (-df['DAYS_BIRTH'] / 365.25).astype(int)
    df = df.drop('DAYS_BIRTH', axis=1)

if 'DAYS_EMPLOYED' in df.columns:
    # Gestion des valeurs aberrantes (365243 = pas d'emploi)
    df['EMPLOYMENT_YEARS'] = -df['DAYS_EMPLOYED'] / 365.25
    df.loc[df['EMPLOYMENT_YEARS'] > 50, 'EMPLOYMENT_YEARS'] = 0  # Pas d'emploi
    df['EMPLOYMENT_YEARS'] = df['EMPLOYMENT_YEARS'].clip(0, 40)
    df = df.drop('DAYS_EMPLOYED', axis=1)

# Encodage des variables cat√©gorielles
categorical_cols = ['CODE_GENDER', 'NAME_EDUCATION_TYPE', 'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS']
for col in categorical_cols:
    if col in df.columns:
        df[f'{col}_ENCODED'] = pd.Categorical(df[col]).codes
        df = df.drop(col, axis=1)

# Calcul de ratios financiers
if 'AMT_INCOME_TOTAL' in df.columns and 'AMT_CREDIT' in df.columns:
    df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL'].replace(0, 1)

if 'AMT_INCOME_TOTAL' in df.columns and 'AMT_ANNUITY' in df.columns:
    df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL'].replace(0, 1)

# Nettoyage des donn√©es
print("\nüßπ NETTOYAGE DES DONN√âES")
print("="*25)

# Suppression des valeurs manquantes
print(f"üìä Valeurs manquantes avant: {df.isnull().sum().sum()}")
df = df.dropna()
print(f"üìä Valeurs manquantes apr√®s: {df.isnull().sum().sum()}")
print(f"üìä √âchantillons restants: {len(df)}")

# Utilisation de TOUTES les donn√©es (pas de limitation)
print(f"üìä Utilisation de TOUTES les donn√©es: {len(df)} √©chantillons")

# D√©finition des features finales
feature_names = [col for col in df.columns if col != 'TARGET']
print(f"\nüìã Features finales: {len(feature_names)}")
for i, feature in enumerate(feature_names, 1):
    print(f"  {i:2d}. {feature}")

print(f"‚úÖ Dataset cr√©√©: {df.shape}")
print(f"üìä Taux de d√©faut: {df['TARGET'].mean():.1%}")
print(f"üìã Variables: {len(feature_names)} features")

# V√©rification des donn√©es
print("\nüìà STATISTIQUES DU DATASET")
print("="*30)
print("Revenus moyens:", f"{df['AMT_INCOME_TOTAL'].mean():,.0f}‚Ç¨")
print("Cr√©dit moyen:", f"{df['AMT_CREDIT'].mean():,.0f}‚Ç¨")
print("Ratio cr√©dit/revenus moyen:", f"{df['CREDIT_INCOME_RATIO'].mean():.1f}x")
print("√Çge moyen:", f"{df['AGE_YEARS'].mean():.0f} ans")
print("Anciennet√© moyenne:", f"{df['EMPLOYMENT_YEARS'].mean():.1f} ans")

# 2. PR√âPARATION DES DONN√âES POUR L'ENTRA√éNEMENT
print("\nüéØ PR√âPARATION POUR L'ENTRA√éNEMENT")
print("="*40)

# S√©paration des features et de la cible
X = df[feature_names].copy()
y = df['TARGET'].copy()

print(f"üìä Features shape: {X.shape}")
print(f"üéØ Target shape: {y.shape}")

# V√©rification des valeurs manquantes
print(f"üîç Valeurs manquantes: {X.isnull().sum().sum()}")

# Division train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚úÖ Train set: {X_train.shape}")
print(f"‚úÖ Test set: {X_test.shape}")

# 3. STANDARDISATION DES DONN√âES
print("\n‚öñÔ∏è STANDARDISATION")
print("="*20)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Standardisation appliqu√©e")

# 4. ENTRA√éNEMENT DU MOD√àLE
print("\nü§ñ ENTRA√éNEMENT DU MOD√àLE")
print("="*30)

# Mod√®le Random Forest compatible scikit-learn 1.3.2
model = RandomForestClassifier(
    n_estimators=100,       # Nombre d'arbres
    max_depth=15,           # Profondeur max
    min_samples_split=10,   # Min √©chantillons pour split
    min_samples_leaf=5,     # Min √©chantillons par feuille
    random_state=42,        # Reproductibilit√©
    n_jobs=-1              # Utiliser tous les CPU
)

print("üîÑ Entra√Ænement en cours...")
model.fit(X_train_scaled, y_train)
print("‚úÖ Mod√®le entra√Æn√©!")

# 5. √âVALUATION DU MOD√àLE
print("\nüìä √âVALUATION")
print("="*15)

# Pr√©dictions
train_score = model.score(X_train_scaled, y_train)
test_score = model.score(X_test_scaled, y_test)

print(f"üéØ Score Train: {train_score:.3f}")
print(f"üéØ Score Test: {test_score:.3f}")

# Pr√©dictions pour AUC
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

print(f"üìà AUC Score: {auc_score:.3f}")

# Importance des features
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nüîù TOP 5 FEATURES IMPORTANTES:")
for i, row in feature_importance.head(5).iterrows():
    print(f"  {row['feature']}: {row['importance']:.3f}")

# 6. SAUVEGARDE DU MOD√àLE
print("\nüíæ SAUVEGARDE")
print("="*15)

# Cr√©ation du dossier model
os.makedirs('model', exist_ok=True)

# Sauvegarde du mod√®le
with open('model/credit_model_v2.pkl', 'wb') as f:
    pickle.dump(model, f)
print("‚úÖ Mod√®le sauvegard√©: model/credit_model_v2.pkl")

# Sauvegarde du scaler
with open('model/scaler_v2.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("‚úÖ Scaler sauvegard√©: model/scaler_v2.pkl")

# Sauvegarde des noms de features
with open('model/feature_names_v2.pkl', 'wb') as f:
    pickle.dump(feature_names, f)
print("‚úÖ Features sauvegard√©es: model/feature_names_v2.pkl")

# Sauvegarde des encoders (vides pour ce mod√®le simple)
encoders = {
    'gender': None,
    'education': None,
    'income_type': None,
    'family_status': None
}

with open('model/encoders_v2.pkl', 'wb') as f:
    pickle.dump(encoders, f)
print("‚úÖ Encoders sauvegard√©s: model/encoders_v2.pkl")

# 7. TEST DU MOD√àLE
print("\nüß™ TEST DU MOD√àLE")
print("="*20)

# Fonction de test adapt√©e aux vraies features
def test_prediction(income, credit_amount, annuity, age, employment_years, 
                   family_members=2, children=0, gender='M', education='Higher education', 
                   income_type='Working', family_status='Married'):
    """Test du mod√®le avec les vraies features"""
    
    # Calcul des ratios
    credit_income_ratio = credit_amount / income
    annuity_income_ratio = annuity / income
    
    # Encodage simple des variables cat√©gorielles (bas√© sur les valeurs typiques)
    gender_encoded = 1 if gender == 'M' else 0
    
    # Encodages simplifi√©s (vous pouvez ajuster selon vos donn√©es)
    education_mapping = {
        'Secondary / secondary special': 0,
        'Higher education': 1,
        'Incomplete higher': 2,
        'Lower secondary': 3,
        'Academic degree': 4
    }
    education_encoded = education_mapping.get(education, 1)
    
    income_type_mapping = {
        'Working': 0,
        'Commercial associate': 1,
        'Pensioner': 2,
        'State servant': 3
    }
    income_type_encoded = income_type_mapping.get(income_type, 0)
    
    family_status_mapping = {
        'Married': 0,
        'Single / not married': 1,
        'Civil marriage': 2,
        'Separated': 3,
        'Widow': 4
    }
    family_status_encoded = family_status_mapping.get(family_status, 0)
    
    # Cr√©ation du vecteur avec exactement les 13 features du mod√®le
    # Dans l'ordre exact: AMT_INCOME_TOTAL, AMT_CREDIT, AMT_ANNUITY, CNT_FAM_MEMBERS, CNT_CHILDREN,
    # AGE_YEARS, EMPLOYMENT_YEARS, CODE_GENDER_ENCODED, NAME_EDUCATION_TYPE_ENCODED,
    # NAME_INCOME_TYPE_ENCODED, NAME_FAMILY_STATUS_ENCODED, CREDIT_INCOME_RATIO, ANNUITY_INCOME_RATIO
    
    features_test = np.array([[
        income,                    # AMT_INCOME_TOTAL
        credit_amount,             # AMT_CREDIT
        annuity,                   # AMT_ANNUITY
        family_members,            # CNT_FAM_MEMBERS
        children,                  # CNT_CHILDREN
        age,                       # AGE_YEARS
        employment_years,          # EMPLOYMENT_YEARS
        gender_encoded,            # CODE_GENDER_ENCODED
        education_encoded,         # NAME_EDUCATION_TYPE_ENCODED
        income_type_encoded,       # NAME_INCOME_TYPE_ENCODED
        family_status_encoded,     # NAME_FAMILY_STATUS_ENCODED
        credit_income_ratio,       # CREDIT_INCOME_RATIO
        annuity_income_ratio       # ANNUITY_INCOME_RATIO
    ]])
    
    print(f"üîç Test avec {features_test.shape[1]} features (attendu: 13)")
    
    # Standardisation
    features_scaled = scaler.transform(features_test)
    
    # Pr√©diction
    risk_proba = model.predict_proba(features_scaled)[0][1]
    risk_score = int(risk_proba * 100)
    
    return risk_score, risk_proba

# Tests avec diff√©rents profils
print("Profil 1 - Bon client:")
score1, proba1 = test_prediction(50000, 150000, 1200, 35, 5)
print(f"  Score: {score1}%, Probabilit√©: {proba1:.3f}")

print("Profil 2 - Client risqu√©:")
score2, proba2 = test_prediction(25000, 200000, 1800, 22, 0)
print(f"  Score: {score2}%, Probabilit√©: {proba2:.3f}")

print("Profil 3 - Client excellent:")
score3, proba3 = test_prediction(80000, 200000, 1500, 40, 10)
print(f"  Score: {score3}%, Probabilit√©: {proba3:.3f}")

print("\nüéâ MOD√àLE CR√â√â AVEC SUCC√àS!")
print("="*30)
print("‚úÖ Mod√®le compatible scikit-learn 1.3.2")
print("‚úÖ Tous les fichiers sauvegard√©s dans model/")
print("‚úÖ Pr√™t pour Streamlit Cloud")
print(f"‚úÖ Version sklearn utilis√©e: {sklearn.__version__}")

print("\nüìÅ Fichiers cr√©√©s:")
print("  - model/credit_model_v2.pkl")
print("  - model/scaler_v2.pkl")
print("  - model/feature_names_v2.pkl")
print("  - model/encoders_v2.pkl")
